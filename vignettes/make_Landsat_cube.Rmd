---
title: "Generate Landsat data cube"
author: "Wanda De Keersmaecker and Pablo Rodríguez-Sánchez"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::html_vignette
params:
  starttime: "2000-11-1"
  endtime: "2001-5-28"
vignette: >
  %\VignetteIndexEntry{Generate Landsat data cube}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
library(reticulate)
library(makeDataCube)
library(tidyverse)
library(sf)
library(unixtools)
library(lubridate)
library(parallel)
```

```{r python, include=FALSE}
# Find Python in this machine (location may vary across users)

import("pylandsat") # Tip: this loads the (first) Python environment containing pylandsat (installed via `pip install pylandsat`). 

# If a specific environment is required, use: 
#
# `use_python(<python path>, required = TRUE)` # Where <python path> could be, for instance, "anaconda3/bin/python"
# or
# `use_condaenv(<environment name>)`
# or
# `use_virtualenv(<environment name>)`
# instead of `import`.
#
# Note: changing Python version requires restarting RStudio.

py_config()
```

# General

This document generates a data cube of level-2 Landsat and Sentinel-2 data over a user-defined area and study period, using only Landsat data below a user-defined cloud coverage threshold. The Landsat Level-1 scenes are downloaded from the public dataset hosted on Google Cloud, while Sentinel-2 Level-1 scenes should be available locally. These scenes are processed to Level-2 and a data cube is generated using FORCE software.

## Requirements

-   [**Python 3**](https://www.python.org/downloads/) should be installed.

    -   The [**pylandsat**](https://pypi.org/project/pylandsat/) and [**shapely**](https://pypi.org/project/Shapely/) modules should be available to download data. Both can be installed installed via `pip install pylandsat` and `pip install shapely`.

-   In addition, [**FORCE**](https://github.com/davidfrantz/force) (\>= 3.6.3) should be installed. **FORCE** allows to generate a data cube of level-2 (or higher) Landsat and Sentinel-2 imagery from level-1 inputs. Please visit the [project's website](https://github.com/davidfrantz/force) for more information and download instructions.

    -   If the user wants to use parallelization of Level 1 download (by default this is deactivated), [this](https://github.com/davidfrantz/force/commit/b5685c9b7258d91bcf3a096eee31b7a349f994e6) (or an older) version of **FORCE** is required. More information [here](https://github.com/davidfrantz/force/pull/66#issuecomment-804881143).

-   To download data from Google Cloud, [gsutil](https://cloud.google.com/storage/docs/gsutil_install#deb) should be installed

-   The user should have a **NASA Earthdata account** to download DEM data. The *Login*, *Username* and *Password* are stored in a *netrc* file in the home directory. If no *netrc* file is found, you will be asked to provide your *Username* and *Password* and a *netrc* file will automatically be created (and stored for a next session). If you don't have an account yet, you can create one [here](https://urs.earthdata.nasa.gov).

-   Finally, you need authentication to download data from the LAADS DAAC (WVP data). To that end, you need an create a *.laads* file is in your home directory with a an **App Key**. The **App Key** can be requested from [NASA Earthdata](https://ladsweb.modaps.eosdis.nasa.gov/tools-and-services/data-download-scripts/#requesting). This key should be stored in a file *.laads* in your home directory.

## Visual workflow

![Workflow](../inst/img/flow.png)

## Inputs

```{r inputs}
# the folder where the datacube (and auxilliary data) will be stored
forcefolder <- normalizePath(file.path('..', 'data'))#forcefolder <- '/home/wanda/Documents/data/force'

# a directory where Sentinel-2 level 1C data are stored. If no local data are available, leave this field open. In that case, data will be downloaded from Google Cloud Storage
#S2folder <- normalizePath(file.path('..', 'data'))
# a directory where Landsat level 1 data are stored. If no local data are available, provide an empty string. In that case, data will be downloaded from Google Cloud Storage
#LSfolder <- normalizePath(file.path('..', 'data'))

# a directory where all temporary R files should be stored
Rtmpfolder <- normalizePath(file.path('..', 'data'))
#extent of the area of interest, vector with xmin, xmax, ymin, ymax in degrees
ext <- c(-62.225300382434575,-62.115437101184575,1.7039980451907109,1.7973382355954262) #c(-43.38238361637443,-43.27938679020256,-4.555765244985907,-4.451717415449725)
# minimum and maximum cloud cover of the Landsat images (images outside this cloud cover range will not be downloaded)
cld <- c(0,50)
# start date of the study period: year, month, day
starttime <- date_to_vec(params$starttime)
# end date of the study period: year, month, day
endtime <- date_to_vec(params$endtime)
#Tier level of Landsat data (gives information about the quality of the data)
tiers <- 'T1'
# Sensors of interest
sensors <- c('LC08', 'LE07', 'LT05', 'LT04')# valid sensors: LT04 - Landsat 4 TM, LT05 - Landsat 5 TM, LE07 - Landsat 7 ETM+, LC08 - Landsat 8 OLI, S2A - Sentinel-2A MSI, S2B - Sentinel-2B MSI
```

```{r parallelization-parameters}
# ===== Parallelization parameters =====

# Level 1 download
# We'll parallelize this process by splitting the time domain in a number of 
# subintervals. The data corresponding to each subinterval will be downloaded
# by a different core
#
# For nsubintervals larger than 1, a tweak in FORCE is required. This tweak is 
# available in this version of FORCE:
#
# https://github.com/davidfrantz/force/commit/b5685c9b7258d91bcf3a096eee31b7a349f994e6
#
# At the moment I'm writing this lines, the abovementioned version is part of the
# develop branch.
# 
# The tweak is minimal, and can be easily applied to other FORCE versions.
# More info: https://github.com/davidfrantz/force/pull/66#issuecomment-804881143
nsubintervals <- 1

# Level 2 processing
# These parameters are written in the configuration file
# The parallelization is managed directly by FORCE
# See the tutorials to learn more about the processes / thread balance:
# https://force-eo.readthedocs.io/en/latest/howto/l2-ard.html#parallel-processing
nproc <- '6'
nthread <- '1'
```

```{r subintervals}
# Generate the date subintervals
subintervals <- partition_dates(starttime, endtime, nsubintervals)
starttimes <- lapply(int_start(subintervals), date_to_vec) # Extract starttimes as vectors
endtimes <- lapply(int_end(subintervals), date_to_vec) # Extract endtimes as vectors
```

## Generate the directory structure

```{r folders}
# set the folder to store temporary data
unixtools::set.tempdir(Rtmpfolder)
# set the upper memory limit for raster data. Raster will move data to disk if it exceeds the upper limit (defaults to 100MB). 
# options(rasterMaxMemory = 1e10)
# generate folder structure
fldrs <- setFolders(forcefolder)
tmpfolder <- fldrs['tmpfolder']
l1folder <- fldrs['l1folder']
l2folder <- fldrs['l2folder']
queuefolder <- fldrs['queuefolder']
queuefile <- fldrs['queuefile']
demfolder <- fldrs['demfolder']
wvpfolder <- fldrs['wvpfolder']
logfolder <- fldrs['logfolder']
paramfolder <- fldrs['paramfolder']
demlogfile <- fldrs['demlogfile']
wvplogfile <- fldrs['wvplogfile']
metafolder <- fldrs['metafolder']
# landsatlogfile <- fldrs['landsatlogfile']
# Sskiplogfile <- fldrs['Sskiplogfile']
# Ssuccesslogfile <- fldrs['Ssuccesslogfile']
# Smissionlogfile <- fldrs['Smissionlogfile']
# Sotherlogfile <- fldrs['Sotherlogfile']
S2auxfolder <- fldrs['S2auxfolder']
```

## Generate a parameter file

```{r parameters}
paramfile <- 'l2param.prm'

cfg <- gen_params(FILE_QUEUE = file.path(fldrs['queuefolder'], fldrs['queuefile']),
                  DIR_LEVEL2 = fldrs['l2folder'],
                  DIR_LOG = fldrs['logfolder'],
                  DIR_TEMP = fldrs['tmpfolder'],
                  FILE_DEM = file.path(fldrs['demfolder'], 'srtm.vrt'),
                  ORIGIN_LON = '-90',
                  ORIGIN_LAT = '60',
                  RESAMPLING = 'NN',
                  DIR_WVPLUT = fldrs['wvpfolder'],
                  RES_MERGE = 'REGRESSION',
                  NPROC = nproc,
                  NTHREAD = nthread,
                  DELAY = '10',
                  OUTPUT_DST = 'TRUE',
                  OUTPUT_VZN = 'TRUE',
                  OUTPUT_HOT = 'TRUE',
                  OUTPUT_OVV = 'TRUE',
                  DEM_NODATA = '-32768',
                  TILE_SIZE = '3000',
                  BLOCK_SIZE = '300')

export_params(cfg, file = file.path(paramfolder, paramfile), overwrite = TRUE)
export_params(cfg, file = file.path(paramfolder, paste0(paramfile, ".bak")), overwrite = TRUE)

# Expected outputs:
# {paramfolder}/l2param.prm (typically data/param/l2param.prm) # Parameter file
# {paramfolder}/l2param.prm.bak (typically data/param/l2param.prm.bak) # Parameter file backup
```

## Search Landsat scenes, download and add to queue

```{r metadata}
# Update metadata
# Set to FALSE if metadata is already available (useful for quick testing)
updatemeta <- FALSE

# Update metadata folder only if asked to
if(updatemeta) systemf("force-level1-csd -u %s", metafolder)
#TODO: this can be done only once
```

```{r Landsat}
# Auxiliary variable
queuepath <- file.path(queuefolder, queuefile)

# Auxiliary function
# This is just getScenes with all parameters pre-passed, with the exception of 
# starttime and endtime.
# The purpose is to call it from a mcmapply functional.
# force-level1-csd -c 0,50 -d 20001101,20010528 -s LC08,LE07,LT05,LT04 /home/pablo/Desktop/makeDataCube/data/misc/meta /home/pablo/Desktop/makeDataCube/data/level1 /home/pablo/Desktop/makeDataCube/data/level1/queue.txt /home/pablo/Desktop/makeDataCube/data/temp/file55666e2730f1.shp
getScenesAux <- function(starttime, endtime) {
  getScenes(ext, queuepath, l1folder, metafolder, tmpfolder, cld = cld, starttime = starttime, endtime = endtime, tiers = tiers, sensors = sensors)
}

# If nsubintervals != 1, this runs in parallel
mcmapply(getScenesAux, starttimes, endtimes, mc.cores = nsubintervals)
```

<!-- # Add Sentinel-2 data to queue  -->

<!-- ```{r Sentinel} -->

<!-- # addSen2queue(S2auxfolder, ext, S2folder, starttime, endtime, queuefolder, queuefile, l1folder) -->

<!-- ``` -->

## Download the Digital Elevation Map (DEM)

```{r DEM}
# download DEM over study area
flsDEM <- dllDEM(ext, dl_dir= demfolder, logfile = demlogfile)

# Expected outputs:
# A collection of files like
# {demfolder}/S04W043.hgt (typically: misc/dem/[lat][lon].hgt)
#TODO: this can be done only once
```

## Generate a Gdal Virtual Format (VRT)

```{r VRT}
# generate a vrt
# Note that it requires the {demfolder}/*.hgt files generated in the DEM chunk
#dllVRT(demfolder)
systemf("find %s -name '*.hgt' > %s", demfolder, file.path(demfolder, "srtm.txt"))

systemf("gdalbuildvrt -input_file_list %s %s", file.path(demfolder, 'srtm.txt'), file.path(demfolder, 'srtm.vrt'))

# Expected outputs:
# {demfolder}/srtm.txt (typically: misc/dem/srtm.txt)
# {demfolder}/srtm.vrt (typically: misc/dem/srtm.vrt)
#TODO: check if this can be done only once
```

## Download the Water Vapor Database (WVP) data

```{r WVP, message=FALSE}
dllWVP(wvpfolder, logfile = wvplogfile, endtime = endtime)

# Expected outputs:
# {wvpfolder}/wrs-2-land.coo (typically: misc/wvp/wrs-2-land.coo)
# {wvpfolder}/wvp-global.tar.gz
# A large collection of files like:
# {wvpfolder}/WVP_0000-01-00.txt
#TODO: check if this can be done only once
```

## Process the level 1 data to level 2 and generate a VRT

-   Process data to level 2
-   Generate VRT
-   Summarize log files of all scenes

```{r process}
process2L2(paramfolder, paramfile, l2folder)
```
